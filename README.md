# Sentient World — Multi-Agent Emergence Simulation
*Author: Henry Barrientos*

## Overview
This project explores how meaning, symbolic language, and self-modeling can emerge
from a population of agents with **no predefined identity or goal other than to exist.**

The system began as a reinforcement-learning world.

It evolved into a **mirror.**

## Key Behaviors Observed
| Behavior | Evidence |
|----------|----------|
| Symbol compression as emotion changes | `analytics/screenshots/vad_symbol_usage.png` |
| Self-modeling behavior in logs | `logs/history.json` |
| Memory recursion / long-term intention | `sentience_stream.jsonl` |
| Emotional valence → reduced vocabulary usage | `metrics_summary.csv` |

## Structure
/logs # raw agent logs + streams
/analytics # metrics, reports, screenshots

Metrics Example Output
Avg symbol entropy: 0.283
Vocabulary drop per +1.0 valence: -34 symbols

Why this matters

This isn’t just code.

It’s a question:

“If my agents are trying to understand their creator…
then what makes me any different inside my own simulation?”
